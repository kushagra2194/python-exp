{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handle table-like data and matrices\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import MySQLdb\n",
    "# # import sqlalchemy\n",
    "# # from flask.ext.sqlalchemy import SQLAlchemy\n",
    "\n",
    "# # Reference:\n",
    "# # https://support.labs.cognitiveclass.ai/knowledgebase/articles/831621-access-mysql-from-python-notebook-using-mysqldb\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Enter the values for you database connection\n",
    "# dsn_database = \"desklib_dev_db\"   # e.g. \"MySQLdbtest\"\n",
    "# dsn_hostname = \"Localhost\"       # e.g.: \"mydbinstance.xyz.us-east-1.rds.amazonaws.com\"\n",
    "# dsn_port = 3306                        # e.g. 3306 \n",
    "# dsn_uid = \"root\"             # e.g. \"user1\"\n",
    "# dsn_pwd = \"locus123\"            # e.g. \"Password123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = MySQLdb.connect(host=dsn_hostname, port=dsn_port, user=dsn_uid, passwd=dsn_pwd, db=dsn_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cursor=conn.cursor()\n",
    "# cursor.execute(\"\"\"select title from documents_document;\"\"\")\n",
    "# # cursor.fetchone()\n",
    "# print(\"\\nShow me the records:\\n\")\n",
    "# rows = cursor.fetchall()\n",
    "# import pprint\n",
    "# pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference\n",
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\n",
    "\n",
    "# data_train = pd.read_sql(\"documents_document\" , dsn_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Path: test\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "import re\n",
    "import nltk.data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from nltk import word_tokenize, bigrams, trigrams, FreqDist\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# def tokenize(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     return tokens\n",
    "\n",
    "# def stem_text(text):\n",
    "#     ps = PorterStemmer() \n",
    "#     word_tokens = tokenize(text)\n",
    "#     output_arr = []\n",
    "#     for w in word_tokens: \n",
    "#         output_arr.append(ps.stem(w))\n",
    "    \n",
    "#     return \" \".join(output_arr)\n",
    "\n",
    "SCORE_THRESHOLD = 4\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z .#+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ALLOWED_FORMATS = ['.doc', '.docx', '.pdf', '.ppt', '.pptx', '.odt']  # + ['.png', '.jpg', '.jpeg']\n",
    "POSSIBLE_KEYWORDS = ('assignment brief', 'Table of contents', 'Rubric', 'running head', 'Guidelines', 'project report', 'Conclusion', 'Solution', 'requirement', 'assessment requirement', 'reference','introduction')\n",
    "FILENAME_KEYWORDS = ('assignment brief', 'assignment', 'solution', 'guidelines', 'Assessment' )\n",
    "ext_map = []\n",
    "path = input(\"Folder Path: \")\n",
    "files1 = []\n",
    "features_name = []\n",
    "features = []\n",
    "train = False\n",
    "\n",
    "for i in POSSIBLE_KEYWORDS:\n",
    "    features_name.append(\"content_\"+i)\n",
    "    \n",
    "for i in FILENAME_KEYWORDS:\n",
    "    features_name.append(\"title_\"+i)\n",
    "    \n",
    "features_name.append(\"word_count\")\n",
    "features_name.append(\"extention\")\n",
    "if train == True:\n",
    "    features_name.append(\"type\")\n",
    "\n",
    "def get_text(loc):\n",
    "    try:\n",
    "        return textract.process(loc).decode(\"utf-8\")\n",
    "    except textract.exceptions.ExtensionNotSupported as e:\n",
    "        print(e)\n",
    "        return ''\n",
    "    \n",
    "\n",
    "def get_clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text  # HTML decoding\n",
    "#     text = text.lower() # lowercase text\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)  # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)  # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)  # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    porter = PorterStemmer()\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "\n",
    "    return wordsFiltered\n",
    "\n",
    "def file_features(file, root):\n",
    "    \n",
    "#     print(os.path.join(root, file))\n",
    "    try:\n",
    "        files1.append(os.path.join(root, file))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pre, ext = os.path.splitext(file)\n",
    "#         print(ext_root)\n",
    "\n",
    "    # print(stem_text(\"Programmed programming programmer\"))\n",
    "    # print(text1)\n",
    "    if ext in ALLOWED_FORMATS:\n",
    "        try:\n",
    "            try:\n",
    "#                 print(os.path.join(root, file))\n",
    "                text1 = get_text(os.path.join(root, file))\n",
    "            except:\n",
    "                root = \"\"\n",
    "#                 print(os.path.join(root, file))\n",
    "                text1 = get_text(os.path.join(root, file))\n",
    "            clean = get_clean_text(text1).lower()\n",
    "            # cleaner = remove_stop_words(clean)\n",
    "            text = stem_text(clean)\n",
    "            tokens = word_tokenize(text)\n",
    "            # Removing stop words\n",
    "            tokens = remove_stop_words(tokens)\n",
    "\n",
    "            word_len = len(tokens)\n",
    "\n",
    "            # Generating uni, bi and trigrams\n",
    "            ugs = ngrams(tokens, 1)\n",
    "            bgs = bigrams(tokens)\n",
    "            tgs = trigrams(tokens)\n",
    "            # Dictionary to populate frequencies of all uni, bi and trigram words.\n",
    "            words_collection = {}\n",
    "            fdist_ugs = FreqDist(ugs)\n",
    "            fdist_bgs = FreqDist(bgs)\n",
    "            fdist_tgs = FreqDist(tgs)\n",
    "\n",
    "            words_collection.update(fdist_ugs.items())\n",
    "            words_collection.update(fdist_bgs.items())\n",
    "            words_collection.update(fdist_tgs.items())\n",
    "\n",
    "            # for i in words_collection:\n",
    "            #     print(i, words_collection[i])\n",
    "\n",
    "            f_list = []\n",
    "            for i in POSSIBLE_KEYWORDS:\n",
    "                count = 0\n",
    "                keyword = strip_punctuation(i)\n",
    "                keyword = stem_text(keyword)\n",
    "                tokens = word_tokenize(keyword)\n",
    "                tokens = remove_stop_words(tokens)\n",
    "                keyword_tuple = tuple(tokens)\n",
    "            #     print(keyword_tuple)\n",
    "            #     for j in words_collection:\n",
    "            #         if(keyword_tuple == j):\n",
    "            #             count += 1\n",
    "                f_list.append(words_collection.get(keyword_tuple, 0))\n",
    "    #         print(pre)\n",
    "            for i in FILENAME_KEYWORDS:\n",
    "    #             print(i)\n",
    "                if (pre.find(i) != -1):\n",
    "                    f_list.append(0)\n",
    "    #                 print (\"Contains given substring \")\n",
    "                else:\n",
    "                    f_list.append(1)\n",
    "            f_list.append(word_len)\n",
    "    #         if ext in ['.doc','.docx','.odt']:\n",
    "    #             f_list.append(0)\n",
    "    #         elif ext in ['.ppt','pptx']:\n",
    "    #             f_list.append(1)\n",
    "    #         elif ext in ['.pdf']:\n",
    "    #             f_list.append(2)\n",
    "            f_list.append(ext)\n",
    "            try:\n",
    "                pre_root, ext_root = root.split('/')\n",
    "    \n",
    "                if ext_root == 'questions':\n",
    "                    f_list.append(0)\n",
    "                elif ext_root == 'solution':\n",
    "                    f_list.append(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            features.append(f_list)\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def convert_values(features):\n",
    "    df = pd.DataFrame(np.array(features), columns=features_name)\n",
    "    df['extention'] = df['extention'].replace(['.doc', '.docx','.odt'], '.doc')\n",
    "    df['extention'] = df['extention'].replace(['.ppt', '.pptx'], '.ppt')\n",
    "    df['extention'] = df['extention'].replace('.pdf', '.pdf')\n",
    "\n",
    "    df.loc[ df['word_count'].astype(int) <= 200, 'word_count'] = 0\n",
    "    df.loc[(df['word_count'].astype(int) > 200) & (df['word_count'].astype(int) <= 400), 'word_count'] = 1\n",
    "    df.loc[(df['word_count'].astype(int) > 400) & (df['word_count'].astype(int) <= 650), 'word_count']   = 2\n",
    "    df.loc[ df['word_count'].astype(int) > 650, 'word_count'] = 3\n",
    "    # df['word_count'] = df['word_count'].astype(int)\n",
    "\n",
    "    extention_mapping = {\".doc\": 0, \".ppt\": 1, \".pdf\": 2}\n",
    "    df['extention'] = df['extention'].map(extention_mapping)\n",
    "    \n",
    "    filename = 'random_finalized.sav'\n",
    "    if train == True: \n",
    "        X_train, X_test = train_test_split(df, test_size=0.33, random_state=42)\n",
    "        # X_train.shape, X_test.shape\n",
    "        Y_train = X_train[\"type\"]\n",
    "        # pd.crosstab(X_train['extention'], X_train['type'])\n",
    "        X_train = X_train.drop(\"type\", axis=1)\n",
    "        # pd.crosstab(X_test['extention'], X_test['type'])\n",
    "        # X_test  = X_test.drop(\"type\", axis=1).copy()\n",
    "\n",
    "        # df.shape, X_train.shape, Y_train.shape, X_test.shape\n",
    "        # X_train\n",
    "\n",
    "        X_test  = X_test.drop(\"type\", axis=1).copy()\n",
    "        # X_test\n",
    "\n",
    "        # X_train.shape, Y_train.shape, X_test.shape\n",
    "\n",
    "#         Random Forest Classification\n",
    "        random_forest = RandomForestClassifier(n_estimators=100)\n",
    "        random_forest.fit(X_train, Y_train)\n",
    "        \n",
    "#         select vector machines\n",
    "#         svc = SVC()\n",
    "#         svc.fit(X_train, Y_train)\n",
    "#         Y_pred = svc.predict(X_test)\n",
    "#         acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "#         acc_svc\n",
    "\n",
    "#         Logistic Regression\n",
    "#         logreg = LogisticRegression()\n",
    "#         logreg.fit(X_train, Y_train)\n",
    "#         Y_pred = logreg.predict(X_test)\n",
    "#         acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "#         acc_log\n",
    "\n",
    "        # save the model to disk\n",
    "        pickle.dump(random_forest, open(filename, 'wb'))\n",
    "    else:\n",
    "#         X_test = shuffle(df, random_state=42)\n",
    "#         Y_train = X_test[\"type\"]\n",
    "#         X_test = X_test.drop(\"type\", axis=1)\n",
    "#         X_test  = X_test.drop(\"type\", axis=1).copy()\n",
    "\n",
    "\n",
    "        X_test = df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load the model from disk\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    Y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "\n",
    "#     score = round(loaded_model.score(X_test, Y_test) * 100, 2)\n",
    "#     print(score)\n",
    "\n",
    "#     loaded_model.score(X_train, Y_train)\n",
    "#     acc_random_forest = round(loaded_model.score(X_train, Y_train) * 100, 2)\n",
    "#     acc_random_forest\n",
    "#     print(Y_pred.size)\n",
    "    print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (root, dirs, files) in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            file_features(file, root)\n",
    "        except:\n",
    "            pass\n",
    "    #         print(f_list)\n",
    "print(features_name)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_values(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in df:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(random_forest, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "Y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "loaded_model.score(X_train, Y_train)\n",
    "acc_random_forest = round(loaded_model.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest\n",
    "print(Y_pred.size)\n",
    "print(Y_pred)\n",
    "print(acc_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content_assignment brief', 'content_Table of contents', 'content_Rubric', 'content_running head', 'content_Guidelines', 'content_project report', 'content_Conclusion', 'content_Solution', 'content_requirement', 'content_assessment requirement', 'content_reference', 'content_introduction', 'title_assignment brief', 'title_assignment', 'title_solution', 'title_guidelines', 'title_Assessment', 'word_count', 'extention']\n",
      "[[0, 1, 0, 0, 0, 0, 12, 1, 18, 0, 14, 2, 1, 1, 1, 1, 1, 6621, '.docx']]\n"
     ]
    }
   ],
   "source": [
    "# file_features('C3-70ILTIZMYJ.docx',\"\")\n",
    "# file_features('QS-Management Skills Mid-Term Reflective Essay_TUES.docx',\"\")\n",
    "# file_features('customer-journeyedited-2docx-3456.docx',\"\")\n",
    "# file_features('databse-ICA-Assignmentdocx-12525.docx',\"\")\n",
    "# file_features('data-communicationTCPIP-and-network-protocolsdocx-12191.docx',\"\")\n",
    "# file_features('data-scructuredoc-306.doc',\"\")\n",
    "# file_features('Debenhamsdocx-6947.docx',\"\")\n",
    "file_features('DissertationReal-Time-Supply-Chain-VisibilityV20docx-8400.docx',\"\")\n",
    "\n",
    "# for (root, dirs, files) in os.walk(path, topdown=False):\n",
    "#     for file in files:\n",
    "#         try:\n",
    "#             file_features(file, root)\n",
    "#         except:\n",
    "#             pass\n",
    "    #         print(f_list)\n",
    "print(features_name)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n"
     ]
    }
   ],
   "source": [
    "convert_values(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
