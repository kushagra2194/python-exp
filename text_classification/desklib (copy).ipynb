{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handle table-like data and matrices\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import MySQLdb\n",
    "# # import sqlalchemy\n",
    "# # from flask.ext.sqlalchemy import SQLAlchemy\n",
    "\n",
    "# # Reference:\n",
    "# # https://support.labs.cognitiveclass.ai/knowledgebase/articles/831621-access-mysql-from-python-notebook-using-mysqldb\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Enter the values for you database connection\n",
    "# dsn_database = \"desklib_dev_db\"   # e.g. \"MySQLdbtest\"\n",
    "# dsn_hostname = \"Localhost\"       # e.g.: \"mydbinstance.xyz.us-east-1.rds.amazonaws.com\"\n",
    "# dsn_port = 3306                        # e.g. 3306 \n",
    "# dsn_uid = \"root\"             # e.g. \"user1\"\n",
    "# dsn_pwd = \"locus123\"            # e.g. \"Password123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = MySQLdb.connect(host=dsn_hostname, port=dsn_port, user=dsn_uid, passwd=dsn_pwd, db=dsn_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cursor=conn.cursor()\n",
    "# cursor.execute(\"\"\"select title from documents_document;\"\"\")\n",
    "# # cursor.fetchone()\n",
    "# print(\"\\nShow me the records:\\n\")\n",
    "# rows = cursor.fetchall()\n",
    "# import pprint\n",
    "# pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference\n",
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\n",
    "\n",
    "# data_train = pd.read_sql(\"documents_document\" , dsn_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Path: files\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "import re\n",
    "import nltk.data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from nltk import word_tokenize, bigrams, trigrams, FreqDist\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# def tokenize(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     return tokens\n",
    "\n",
    "# def stem_text(text):\n",
    "#     ps = PorterStemmer() \n",
    "#     word_tokens = tokenize(text)\n",
    "#     output_arr = []\n",
    "#     for w in word_tokens: \n",
    "#         output_arr.append(ps.stem(w))\n",
    "    \n",
    "#     return \" \".join(output_arr)\n",
    "\n",
    "SCORE_THRESHOLD = 4\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z .#+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ALLOWED_FORMATS = ['.doc', '.docx', '.pdf', '.ppt', '.pptx', '.odt']  # + ['.png', '.jpg', '.jpeg']\n",
    "POSSIBLE_KEYWORDS = ('assignment brief', 'Table of contents', 'Rubric', 'name', 'Guidelines', 'project report', 'Conclusion', 'Solution', 'requirement', 'assessment requirement')\n",
    "FILENAME_KEYWORDS = ('assignment brief', 'assignment', 'solution', 'guidelines', 'Assessment' )\n",
    "ext_map = []\n",
    "path = input(\"Folder Path: \")\n",
    "files1 = []\n",
    "features_name = []\n",
    "features = []\n",
    "\n",
    "for i in POSSIBLE_KEYWORDS:\n",
    "    features_name.append(\"content_\"+i)\n",
    "    \n",
    "for i in FILENAME_KEYWORDS:\n",
    "    features_name.append(\"title_\"+i)\n",
    "    \n",
    "features_name.append(\"word_count\")\n",
    "features_name.append(\"extention\")\n",
    "features_name.append(\"type\")\n",
    "\n",
    "def get_text(loc):\n",
    "    try:\n",
    "        return textract.process(loc).decode(\"utf-8\")\n",
    "    except textract.exceptions.ExtensionNotSupported as e:\n",
    "        print(e)\n",
    "        return ''\n",
    "    \n",
    "\n",
    "def get_clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text  # HTML decoding\n",
    "#     text = text.lower() # lowercase text\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)  # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)  # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)  # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    porter = PorterStemmer()\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "\n",
    "    return wordsFiltered\n",
    "\n",
    "def file_features(file):\n",
    "    \n",
    "#     print(os.path.join(root, file))\n",
    "    try:\n",
    "        files1.append(os.path.join(root, file))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pre, ext = os.path.splitext(file)\n",
    "#         print(ext_root)\n",
    "\n",
    "    # print(stem_text(\"Programmed programming programmer\"))\n",
    "    # print(text1)\n",
    "    if ext in ALLOWED_FORMATS:\n",
    "        try:\n",
    "            try:\n",
    "                print(os.path.join(root, file))\n",
    "                text1 = get_text(os.path.join(root, file))\n",
    "            except:\n",
    "                root = \"\"\n",
    "                print(os.path.join(root, file))\n",
    "                text1 = get_text(os.path.join(root, file))\n",
    "            clean = get_clean_text(text1).lower()\n",
    "            # cleaner = remove_stop_words(clean)\n",
    "            text = stem_text(clean)\n",
    "            tokens = word_tokenize(text)\n",
    "            # Removing stop words\n",
    "            tokens = remove_stop_words(tokens)\n",
    "\n",
    "            word_len = len(tokens)\n",
    "\n",
    "            # Generating uni, bi and trigrams\n",
    "            ugs = ngrams(tokens, 1)\n",
    "            bgs = bigrams(tokens)\n",
    "            tgs = trigrams(tokens)\n",
    "            # Dictionary to populate frequencies of all uni, bi and trigram words.\n",
    "            words_collection = {}\n",
    "            fdist_ugs = FreqDist(ugs)\n",
    "            fdist_bgs = FreqDist(bgs)\n",
    "            fdist_tgs = FreqDist(tgs)\n",
    "\n",
    "            words_collection.update(fdist_ugs.items())\n",
    "            words_collection.update(fdist_bgs.items())\n",
    "            words_collection.update(fdist_tgs.items())\n",
    "\n",
    "            # for i in words_collection:\n",
    "            #     print(i, words_collection[i])\n",
    "\n",
    "            f_list = []\n",
    "            for i in POSSIBLE_KEYWORDS:\n",
    "                count = 0\n",
    "                keyword = strip_punctuation(i)\n",
    "                keyword = stem_text(keyword)\n",
    "                tokens = word_tokenize(keyword)\n",
    "                tokens = remove_stop_words(tokens)\n",
    "                keyword_tuple = tuple(tokens)\n",
    "            #     print(keyword_tuple)\n",
    "            #     for j in words_collection:\n",
    "            #         if(keyword_tuple == j):\n",
    "            #             count += 1\n",
    "                f_list.append(words_collection.get(keyword_tuple, 0))\n",
    "    #         print(pre)\n",
    "            for i in FILENAME_KEYWORDS:\n",
    "    #             print(i)\n",
    "                if (pre.find(i) != -1):\n",
    "                    f_list.append(0)\n",
    "    #                 print (\"Contains given substring \")\n",
    "                else:\n",
    "                    f_list.append(1)\n",
    "            f_list.append(word_len)\n",
    "    #         if ext in ['.doc','.docx','.odt']:\n",
    "    #             f_list.append(0)\n",
    "    #         elif ext in ['.ppt','pptx']:\n",
    "    #             f_list.append(1)\n",
    "    #         elif ext in ['.pdf']:\n",
    "    #             f_list.append(2)\n",
    "            f_list.append(ext)\n",
    "            try:\n",
    "                if ext_root == 'questions':\n",
    "                    f_list.append(0)\n",
    "                else:\n",
    "                    f_list.append(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            features.append(f_list)\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capstone-Project-Requirement-part-2docx-16298.docx\n"
     ]
    },
    {
     "ename": "MissingFileError",
     "evalue": "The file \"Capstone-Project-Requirement-part-2docx-16298.docx\" can not be found.\nIs this the right path/to/file/you/want/to/extract.docx?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc4a4b380ac1>\u001b[0m in \u001b[0;36mfile_features\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'root' referenced before assignment",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMissingFileError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ab6e49f9f912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpre_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mfile_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#         print(f_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dc4a4b380ac1>\u001b[0m in \u001b[0;36mfile_features\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# cleaner = remove_stop_words(clean)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dc4a4b380ac1>\u001b[0m in \u001b[0;36mget_text\u001b[0;34m(loc)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtensionNotSupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_practice/python-exp/desklib/venv/lib/python3.5/site-packages/textract/parsers/__init__.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(filename, encoding, extension, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# make sure the filename exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# get the filename extension, which is something like .docx for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingFileError\u001b[0m: The file \"Capstone-Project-Requirement-part-2docx-16298.docx\" can not be found.\nIs this the right path/to/file/you/want/to/extract.docx?"
     ]
    }
   ],
   "source": [
    "for (root, dirs, files) in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        pre_root, ext_root = root.split('/')\n",
    "        file_features(file)\n",
    "    #         print(f_list)\n",
    "print(features_name)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array(features), columns=features_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in df:\n",
    "df['extention'] = df['extention'].replace(['.doc', '.docx','.odt'], '.doc')\n",
    "df['extention'] = df['extention'].replace(['.ppt', '.pptx'], '.ppt')\n",
    "df['extention'] = df['extention'].replace('.pdf', '.pdf')\n",
    "\n",
    "df.loc[ df['word_count'].astype(int) <= 200, 'word_count'] = 0\n",
    "df.loc[(df['word_count'].astype(int) > 200) & (df['word_count'].astype(int) <= 400), 'word_count'] = 1\n",
    "df.loc[(df['word_count'].astype(int) > 400) & (df['word_count'].astype(int) <= 650), 'word_count']   = 2\n",
    "df.loc[ df['word_count'].astype(int) > 650, 'word_count'] = 3\n",
    "# df['word_count'] = df['word_count'].astype(int)\n",
    "\n",
    "extention_mapping = {\".doc\": 0, \".ppt\": 1, \".pdf\": 2}\n",
    "df['extention'] = df['extention'].map(extention_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, test_size=0.33, random_state=42)\n",
    "# X_train.shape, X_test.shape\n",
    "Y_train = X_train[\"type\"]\n",
    "# pd.crosstab(X_train['extention'], X_train['type'])\n",
    "X_train = X_train.drop(\"type\", axis=1)\n",
    "# pd.crosstab(X_test['extention'], X_test['type'])\n",
    "# X_test  = X_test.drop(\"type\", axis=1).copy()\n",
    "\n",
    "# df.shape, X_train.shape, Y_train.shape, X_test.shape\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = X_test.drop(\"type\", axis=1).copy()\n",
    "# X_test\n",
    "\n",
    "# X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(random_forest, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "Y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "loaded_model.score(X_train, Y_train)\n",
    "acc_random_forest = round(loaded_model.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest\n",
    "print(Y_pred.size)\n",
    "print(Y_pred)\n",
    "print(acc_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_features('Customer-behaviordocx-15262.docx')\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
